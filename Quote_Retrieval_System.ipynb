{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX+DpOFnJAtPiONzpx3zhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manya123-max/Assesments/blob/main/Quote_Retrieval_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 - Package Installation:"
      ],
      "metadata": {
        "id": "myDDaqmh3j5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Packages\n",
        "print(\"Installing required packages...\")\n",
        "\n",
        "!pip install -q sentence-transformers datasets transformers torch torchvision torchaudio\n",
        "!pip install -q faiss-cpu pandas numpy scikit-learn\n",
        "!pip install -q gradio\n",
        "!pip install -q huggingface_hub accelerate fsspec\n",
        "\n",
        "print(\"All packages installed successfully!\")\n",
        "print(\"Please restart runtime if prompted, then proceed to Step 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUVOhcGN3jjp",
        "outputId": "d5c915ba-a89f-48a3-cbe6-28922e8f68af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "All packages installed successfully!\n",
            "Please restart runtime if prompted, then proceed to Step 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 -Import Libraries and Setup"
      ],
      "metadata": {
        "id": "cLzTUQ5G3z8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries and Setup\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTHIojRP342a",
        "outputId": "801a69c0-49f3-4a1f-b9ac-8a3c688fe3d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3- Data Processing Class"
      ],
      "metadata": {
        "id": "9u9UZ3z83_LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quote Data Processor Class\n",
        "class ColabQuoteDataProcessor:\n",
        "    def __init__(self):\n",
        "        self.dataset = None\n",
        "        self.processed_data = None\n",
        "\n",
        "    def load_data(self, max_samples=5000):\n",
        "        \"\"\"Load dataset using pandas read_json from HuggingFace\"\"\"\n",
        "        print(\" Loading dataset from HuggingFace...\")\n",
        "        try:\n",
        "            # Load dataset using the specified method\n",
        "            df = pd.read_json(\"hf://datasets/Abirate/english_quotes/quotes.jsonl\", lines=True)\n",
        "            print(f\"Dataset loaded successfully. Total size: {len(df)}\")\n",
        "\n",
        "            # Limit dataset size for Colab memory constraints\n",
        "            if len(df) > max_samples:\n",
        "                df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
        "                print(f\"Randomly sampled {max_samples} quotes for Colab optimization\")\n",
        "\n",
        "            # Store as dataset format for compatibility\n",
        "            self.dataset = {\"train\": df}\n",
        "            print(f\" Dataset ready with {len(df)} quotes\")\n",
        "            return self.dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading dataset with pandas method: {e}\")\n",
        "            print(\" Trying alternative HuggingFace datasets library...\")\n",
        "            try:\n",
        "                # Fallback to datasets library\n",
        "                from datasets import load_dataset\n",
        "                dataset = load_dataset(\"Abirate/english_quotes\")\n",
        "                df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "                if len(df) > max_samples:\n",
        "                    df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
        "                    print(f\"Fallback: Limited to {max_samples} samples\")\n",
        "\n",
        "                self.dataset = {\"train\": df}\n",
        "                return self.dataset\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\" Fallback also failed: {e2}\")\n",
        "                # Create sample data if both methods fail\n",
        "                return self.create_sample_data()\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Create sample data if dataset loading fails\"\"\"\n",
        "        print(\"üîß Creating sample dataset as fallback...\")\n",
        "        sample_quotes = [\n",
        "            {\"quote\": \"The only way to do great work is to love what you do.\", \"author\": \"Steve Jobs\", \"tags\": [\"motivation\", \"work\", \"success\"]},\n",
        "            {\"quote\": \"Life is what happens to you while you're busy making other plans.\", \"author\": \"John Lennon\", \"tags\": [\"life\", \"philosophy\"]},\n",
        "            {\"quote\": \"The future belongs to those who believe in the beauty of their dreams.\", \"author\": \"Eleanor Roosevelt\", \"tags\": [\"dreams\", \"future\", \"hope\"]},\n",
        "            {\"quote\": \"It is during our darkest moments that we must focus to see the light.\", \"author\": \"Aristotle\", \"tags\": [\"hope\", \"perseverance\"]},\n",
        "            {\"quote\": \"The way to get started is to quit talking and begin doing.\", \"author\": \"Walt Disney\", \"tags\": [\"action\", \"motivation\"]},\n",
        "            {\"quote\": \"Your time is limited, don't waste it living someone else's life.\", \"author\": \"Steve Jobs\", \"tags\": [\"life\", \"authenticity\"]},\n",
        "            {\"quote\": \"If life were predictable it would cease to be life, and be without flavor.\", \"author\": \"Eleanor Roosevelt\", \"tags\": [\"life\", \"unpredictability\"]},\n",
        "            {\"quote\": \"The only impossible journey is the one you never begin.\", \"author\": \"Tony Robbins\", \"tags\": [\"journey\", \"motivation\"]},\n",
        "            {\"quote\": \"In the end, we will remember not the words of our enemies, but the silence of our friends.\", \"author\": \"Martin Luther King Jr.\", \"tags\": [\"friendship\", \"courage\"]},\n",
        "            {\"quote\": \"Success is not final, failure is not fatal: it is the courage to continue that counts.\", \"author\": \"Winston Churchill\", \"tags\": [\"success\", \"failure\", \"courage\"]}\n",
        "        ]\n",
        "\n",
        "        # Create dataset structure\n",
        "        df = pd.DataFrame(sample_quotes)\n",
        "        self.dataset = {\"train\": df}\n",
        "        print(f\"Sample dataset created with {len(df)} quotes\")\n",
        "        return self.dataset\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Clean and preprocess the dataset\"\"\"\n",
        "        print(\"Preprocessing data...\")\n",
        "\n",
        "        # Get DataFrame from dataset\n",
        "        if isinstance(self.dataset['train'], pd.DataFrame):\n",
        "            df = self.dataset['train'].copy()\n",
        "        else:\n",
        "            df = pd.DataFrame(self.dataset['train'])\n",
        "\n",
        "        print(f\"Original dataset shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Display sample data\n",
        "        print(\"Sample data:\")\n",
        "        print(df.head(2))\n",
        "\n",
        "        # Handle missing values\n",
        "        print(\"Handling missing values...\")\n",
        "        initial_size = len(df)\n",
        "        df = df.dropna(subset=['quote', 'author'])\n",
        "        print(f\"Removed {initial_size - len(df)} rows with missing quote/author\")\n",
        "\n",
        "        # Clean text\n",
        "        df['quote_clean'] = df['quote'].astype(str).str.strip()\n",
        "        df['author_clean'] = df['author'].astype(str).str.strip()\n",
        "\n",
        "        # Handle tags - check if tags column exists and handle different formats\n",
        "        if 'tags' in df.columns:\n",
        "            print(\" Processing tags column...\")\n",
        "            df['tags'] = df['tags'].apply(lambda x:\n",
        "                x if isinstance(x, list)\n",
        "                else [x] if isinstance(x, str) and x.strip()\n",
        "                else []\n",
        "            )\n",
        "        else:\n",
        "            print(\"No tags column found, creating empty tags\")\n",
        "            df['tags'] = [[] for _ in range(len(df))]\n",
        "\n",
        "        # Create search text for embedding\n",
        "        df['search_text'] = df.apply(\n",
        "            lambda row: f\"Quote: {row['quote']} Author: {row['author']} Tags: {', '.join(row['tags']) if row['tags'] else 'no tags'}\",\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        self.processed_data = df.reset_index(drop=True)\n",
        "        print(f\"Data preprocessing completed!\")\n",
        "        print(f\"Final dataset size: {len(df)} quotes\")\n",
        "        print(f\"Sample search text: {df['search_text'].iloc[0][:100]}...\")\n",
        "\n",
        "        return df\n",
        "\n",
        "print(\"Data Processing Class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w2THIXR4Cgs",
        "outputId": "1b942bbb-af6a-4b11-fd5e-a9c713012d68"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Processing Class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 - Embedding Model Class"
      ],
      "metadata": {
        "id": "s7bq7iKf4WX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quote Embedding Model Class\n",
        "class ColabQuoteEmbeddingModel:\n",
        "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.device = device\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the sentence transformer model\"\"\"\n",
        "        print(f\" Loading model: {self.model_name}\")\n",
        "        try:\n",
        "            self.model = SentenceTransformer(self.model_name, device=str(self.device))\n",
        "            print(f\" Model loaded successfully on {self.device}\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading model: {e}\")\n",
        "            # Fallback to CPU\n",
        "            self.model = SentenceTransformer(self.model_name, device='cpu')\n",
        "            print(\" Loaded model on CPU\")\n",
        "        return self.model\n",
        "\n",
        "print(\" Embedding Model Class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jXIQxF24YzQ",
        "outputId": "add4c80b-b9a5-4267-f519-a70b1774ed83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Embedding Model Class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 - RAG Pipeline Class"
      ],
      "metadata": {
        "id": "EKkvl3Ri4lm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quote RAG Pipeline Class\n",
        "class ColabQuoteRAGPipeline:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.index = None\n",
        "        self.quotes_data = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def create_embeddings(self, quotes_data):\n",
        "        \"\"\"Create embeddings for all quotes\"\"\"\n",
        "        print(\"Creating embeddings...\")\n",
        "        self.quotes_data = quotes_data.reset_index(drop=True)\n",
        "\n",
        "        # Generate embeddings in batches to manage memory\n",
        "        texts = quotes_data['search_text'].tolist()\n",
        "        batch_size = 32\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_embeddings = self.embedding_model.encode(batch_texts, convert_to_tensor=False)\n",
        "            embeddings_list.append(batch_embeddings)\n",
        "            print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} texts\")\n",
        "\n",
        "        self.embeddings = np.vstack(embeddings_list)\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings.astype('float32'))\n",
        "\n",
        "        print(f\"Index created with {self.index.ntotal} vectors\")\n",
        "        return self.index\n",
        "\n",
        "    def retrieve_quotes(self, query, top_k=5):\n",
        "        \"\"\"Retrieve relevant quotes for a query\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not created. Call create_embeddings() first.\")\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search\n",
        "        similarities, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "        # Get results\n",
        "        results = []\n",
        "        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):\n",
        "            if idx < len(self.quotes_data):\n",
        "                quote_data = self.quotes_data.iloc[idx]\n",
        "                results.append({\n",
        "                    'quote': quote_data['quote'],\n",
        "                    'author': quote_data['author'],\n",
        "                    'tags': quote_data['tags'],\n",
        "                    'similarity': float(similarity),\n",
        "                    'rank': i + 1\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def query(self, user_query, top_k=5):\n",
        "        \"\"\"Complete RAG query processing\"\"\"\n",
        "        retrieved_quotes = self.retrieve_quotes(user_query, top_k)\n",
        "        return retrieved_quotes\n",
        "\n",
        "print(\"RAG Pipeline Class defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ltA2ui74i1v",
        "outputId": "4140d4cf-69f5-4a7c-be4a-de53983ff6d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Pipeline Class defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6 - System Initialization:"
      ],
      "metadata": {
        "id": "v0GB4FdR4u34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Process Data\n",
        "print(\"Loading and processing data...\")\n",
        "\n",
        "data_processor = ColabQuoteDataProcessor()\n",
        "dataset = data_processor.load_data(max_samples=1000)  # Reduced for better performance\n",
        "processed_data = data_processor.preprocess_data()\n",
        "\n",
        "print(f\" Data loaded: {len(processed_data)} quotes\")\n",
        "print(\" Ready for next step!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HA1CHmuAqr6",
        "outputId": "216ce9df-ea24-4aec-d01b-552a8bbbf650"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and processing data...\n",
            " Loading dataset from HuggingFace...\n",
            "Dataset loaded successfully. Total size: 2508\n",
            "Randomly sampled 1000 quotes for Colab optimization\n",
            " Dataset ready with 1000 quotes\n",
            "Preprocessing data...\n",
            "Original dataset shape: (1000, 3)\n",
            "Columns: ['quote', 'author', 'tags']\n",
            "Sample data:\n",
            "                                               quote                author  \\\n",
            "0  ‚ÄúIf you never did you should. These things are...             Dr. Seuss   \n",
            "1         ‚ÄúLove all, trust a few, do wrong to none.‚Äù  William Shakespeare,   \n",
            "\n",
            "                             tags  \n",
            "0                         [suess]  \n",
            "1  [do-wrong, love, trust, wrong]  \n",
            "Handling missing values...\n",
            "Removed 0 rows with missing quote/author\n",
            " Processing tags column...\n",
            "Data preprocessing completed!\n",
            "Final dataset size: 1000 quotes\n",
            "Sample search text: Quote: ‚ÄúIf you never did you should. These things are fun and fun is good.‚Äù Author: Dr. Seuss Tags: ...\n",
            " Data loaded: 1000 quotes\n",
            " Ready for next step!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding Model\n",
        "print(\" Loading embedding model...\")\n",
        "\n",
        "embedding_model = ColabQuoteEmbeddingModel()\n",
        "model = embedding_model.load_model()\n",
        "\n",
        "print(\" Model loaded successfully!\")\n",
        "print(\" Ready for embedding creation!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv00JK8QAww9",
        "outputId": "187f07f5-33fd-4131-b365-5722597243fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading embedding model...\n",
            " Loading model: all-MiniLM-L6-v2\n",
            " Model loaded successfully on cpu\n",
            " Model loaded successfully!\n",
            " Ready for embedding creation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RAG Pipeline and Embeddings\n",
        "print(\" Creating RAG pipeline and embeddings...\")\n",
        "print(\" This may take a few minutes depending on dataset size...\")\n",
        "\n",
        "rag_pipeline = ColabQuoteRAGPipeline(model)\n",
        "\n",
        "# Add progress tracking\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "rag_pipeline.create_embeddings(processed_data)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\" Embedding creation took {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Store in global variable for easy access\n",
        "rag_system = rag_pipeline\n",
        "\n",
        "print(\" System fully initialized!\")\n",
        "print(f\" Ready to search through {len(processed_data)} quotes\")\n",
        "print(\" Proceed to next cell for testing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbX_jEzLA2LQ",
        "outputId": "cf0a03e3-e265-4222-8b25-508d8c885701"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Creating RAG pipeline and embeddings...\n",
            " This may take a few minutes depending on dataset size...\n",
            "Creating embeddings...\n",
            "Processed 32/1000 texts\n",
            "Processed 64/1000 texts\n",
            "Processed 96/1000 texts\n",
            "Processed 128/1000 texts\n",
            "Processed 160/1000 texts\n",
            "Processed 192/1000 texts\n",
            "Processed 224/1000 texts\n",
            "Processed 256/1000 texts\n",
            "Processed 288/1000 texts\n",
            "Processed 320/1000 texts\n",
            "Processed 352/1000 texts\n",
            "Processed 384/1000 texts\n",
            "Processed 416/1000 texts\n",
            "Processed 448/1000 texts\n",
            "Processed 480/1000 texts\n",
            "Processed 512/1000 texts\n",
            "Processed 544/1000 texts\n",
            "Processed 576/1000 texts\n",
            "Processed 608/1000 texts\n",
            "Processed 640/1000 texts\n",
            "Processed 672/1000 texts\n",
            "Processed 704/1000 texts\n",
            "Processed 736/1000 texts\n",
            "Processed 768/1000 texts\n",
            "Processed 800/1000 texts\n",
            "Processed 832/1000 texts\n",
            "Processed 864/1000 texts\n",
            "Processed 896/1000 texts\n",
            "Processed 928/1000 texts\n",
            "Processed 960/1000 texts\n",
            "Processed 992/1000 texts\n",
            "Processed 1000/1000 texts\n",
            "Index created with 1000 vectors\n",
            " Embedding creation took 92.33 seconds\n",
            " System fully initialized!\n",
            " Ready to search through 1000 quotes\n",
            " Proceed to next cell for testing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7 - Test Search Function"
      ],
      "metadata": {
        "id": "cSKfT1nL41Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Test to Verify System\n",
        "print(\" Testing the system...\")\n",
        "\n",
        "# Simple test function\n",
        "def quick_test():\n",
        "    try:\n",
        "        test_query = \"motivation\"\n",
        "        print(f\" Testing query: '{test_query}'\")\n",
        "\n",
        "        results = rag_system.query(test_query, top_k=2)\n",
        "\n",
        "        if results:\n",
        "            print(\" System is working!\")\n",
        "            for i, result in enumerate(results, 1):\n",
        "                print(f\"{i}. \\\"{result['quote'][:50]}...\\\" - {result['author']}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\" No results found\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "if quick_test():\n",
        "    print(\" System ready for Gradio interface!\")\n",
        "else:\n",
        "    print(\" Please check previous cells for errors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRqm51l744Fy",
        "outputId": "6cfee2ec-b737-4a68-fa60-eb3e9c761926"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing the system...\n",
            " Testing query: 'motivation'\n",
            " System is working!\n",
            "1. \"‚ÄúThe starting point of all achievement is DESIRE. ...\" - Napoleon Hill,\n",
            "2. \"‚ÄúOf course motivation is not permanent. But then, ...\" - Zig Ziglar,\n",
            " System ready for Gradio interface!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main search function for Gradio\n",
        "def search_quotes(query, num_results=5):\n",
        "    \"\"\"Search for quotes based on user query\"\"\"\n",
        "    print(f\" Searching for: '{query}'\")  # Debug print\n",
        "\n",
        "    if not query.strip():\n",
        "        return \" Please enter a valid query.\"\n",
        "\n",
        "    try:\n",
        "        # Query the system\n",
        "        retrieved_quotes = rag_system.query(query, top_k=num_results)\n",
        "\n",
        "        if not retrieved_quotes:\n",
        "            return f\"No quotes found for: '{query}'\"\n",
        "\n",
        "        # Format response nicely\n",
        "        response = f\" **Search Results for:** '{query}'\\n\\n\"\n",
        "\n",
        "        for i, quote in enumerate(retrieved_quotes, 1):\n",
        "            response += f\"**{i}. Quote (Similarity: {quote['similarity']:.3f})**\\n\"\n",
        "            response += f\"üí¨ \\\"{quote['quote']}\\\"\\n\"\n",
        "            response += f\"üë§ **Author:** {quote['author']}\\n\"\n",
        "\n",
        "            if quote['tags']:\n",
        "                response += f\" **Tags:** {', '.join(quote['tags'])}\\n\"\n",
        "\n",
        "            response += \"\\n\" + \"‚îÄ\" * 50 + \"\\n\\n\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Search error: {e}\")  # Debug print\n",
        "        return f\" Search failed: {str(e)}\"\n",
        "\n",
        "print(\" Search function defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kQfvdPhBKQF",
        "outputId": "ba024156-d121-40b7-ee18-566bba60e4e5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Search function defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 - Gradio Interface:"
      ],
      "metadata": {
        "id": "oGTxHm184-bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio Interface (Don't launch yet)\n",
        "def create_interface():\n",
        "    \"\"\"Create a clean Gradio interface\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"Quote Search System\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# Semantic Quote Search System\")\n",
        "        gr.Markdown(\"*Find quotes using natural language queries*\")\n",
        "\n",
        "        # Search Interface\n",
        "        gr.Markdown(\"### Search for Quotes\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Enter your search query\",\n",
        "                    placeholder=\"e.g., 'quotes about love', 'motivation quotes', 'Steve Jobs quotes'\",\n",
        "                    lines=2\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                num_results = gr.Slider(\n",
        "                    label=\"Number of results\",\n",
        "                    minimum=1,\n",
        "                    maximum=10,\n",
        "                    value=5,\n",
        "                    step=1\n",
        "                )\n",
        "\n",
        "        search_btn = gr.Button(\"Search Quotes\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        # Results\n",
        "        results_output = gr.Markdown(\n",
        "            label=\"Search Results\",\n",
        "            value=\"Enter a query and click 'Search Quotes' to see results.\"\n",
        "        )\n",
        "\n",
        "        search_btn.click(\n",
        "            search_quotes,\n",
        "            inputs=[query_input, num_results],\n",
        "            outputs=results_output\n",
        "        )\n",
        "\n",
        "        # Quick Examples\n",
        "        gr.Markdown(\"### Quick Examples\")\n",
        "\n",
        "        example_queries = [\n",
        "            \"quotes about love\",\n",
        "            \"motivational quotes\",\n",
        "            \"Steve Jobs quotes\",\n",
        "            \"quotes about life\",\n",
        "            \"inspirational quotes\",\n",
        "            \"quotes about success\"\n",
        "        ]\n",
        "\n",
        "        with gr.Row():\n",
        "            for query in example_queries[:3]:\n",
        "                gr.Button(query, size=\"sm\").click(\n",
        "                    lambda q=query: q, outputs=query_input\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            for query in example_queries[3:]:\n",
        "                gr.Button(query, size=\"sm\").click(\n",
        "                    lambda q=query: q, outputs=query_input\n",
        "                )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create the interface\n",
        "print(\"Creating Gradio interface...\")\n",
        "demo = create_interface()\n",
        "print(\"Interface created successfully!\")\n",
        "print(\"Ready to launch in next cell!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7ozVZJB5BkR",
        "outputId": "a68a327c-482c-431f-ddfd-ae672692f3f8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Gradio interface...\n",
            "Interface created successfully!\n",
            "Ready to launch in next cell!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9 - Launch Interface"
      ],
      "metadata": {
        "id": "J6IKZgXt5J0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the Gradio Interface\n",
        "print(\"üöÄ Launching Quote Search System Interface...\")\n",
        "\n",
        "# Create and launch the interface\n",
        "demo = create_interface()\n",
        "\n",
        "# Launch with public sharing for Colab\n",
        "demo.launch(\n",
        "    share=True,      # Creates public URL\n",
        "    debug=True,      # Show debug info\n",
        "    height=600,      # Interface height\n",
        "    show_error=True, # Show errors\n",
        "    quiet=False      # Show launch info\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Interface launched successfully!\")\n",
        "print(\"üåê A public URL has been generated for sharing\")\n",
        "print(\"‚èπÔ∏è To stop the interface, interrupt the kernel or restart runtime\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "oOe1WrOC5JDF",
        "outputId": "41446683-efd5-4d55-be61-72bc899089cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching Quote Search System Interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aed85085849f38f46a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aed85085849f38f46a.gradio.live\" width=\"100%\" height=\"600\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://aed85085849f38f46a.gradio.live\n",
            "‚úÖ Interface launched successfully!\n",
            "üåê A public URL has been generated for sharing\n",
            "‚èπÔ∏è To stop the interface, interrupt the kernel or restart runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10 - Stop and Cleanup"
      ],
      "metadata": {
        "id": "H1UC9Clj5PRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STOP the interface and clean up\n",
        "print(\"Stopping the interface...\")\n",
        "\n",
        "try:\n",
        "    # Stop the Gradio interface\n",
        "    demo.close()\n",
        "    print(\"Gradio interface stopped!\")\n",
        "except Exception as e:\n",
        "    print(f\"Interface stop warning: {e}\")\n",
        "\n",
        "try:\n",
        "    # Clear large variables from memory\n",
        "    del rag_system\n",
        "    del rag_pipeline\n",
        "    del embedding_model\n",
        "    del processed_data\n",
        "    del demo\n",
        "    print(\"Large variables cleared from memory!\")\n",
        "except:\n",
        "    print(\"Some variables were already cleared\")\n",
        "\n",
        "# Force garbage collection\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"Cleanup completed!\")\n",
        "print(\"You can now run other code or restart if needed\")\n",
        "print(\"To restart the system, run cells 1-10 again\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPghF2FDBl9j",
        "outputId": "dc129a60-55f4-4b2d-c97a-2d08d85676e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping the interface...\n",
            "Closing server running on port: 7860\n",
            "Gradio interface stopped!\n",
            "Large variables cleared from memory!\n",
            "Cleanup completed!\n",
            "You can now run other code or restart if needed\n",
            "To restart the system, run cells 1-10 again\n"
          ]
        }
      ]
    }
  ]
}